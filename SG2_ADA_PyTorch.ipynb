{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SG2_ADA_PyTorch.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"jG7ZEc_982io"},"source":["# StyleGAN2-ADA-PyTorch\n","\n","---\n","\n","If you find this notebook useful, consider signing up for my [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send me a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZXMA1LGPv6L","executionInfo":{"status":"ok","timestamp":1615043019294,"user_tz":-60,"elapsed":1277,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"6f93c28b-3735-4996-a390-05ab3b6866d9"},"source":["# mount Google Drive on the runtime\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ig5hqcirQ5ka","executionInfo":{"status":"ok","timestamp":1615043019295,"user_tz":-60,"elapsed":1266,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"ebe6d116-239e-4d5d-d4dc-dcb7fd8b9b42"},"source":["# create a symbolic link to a working directory\n","!ln -s /content/gdrive/My\\ Drive/Colab\\ Notebooks/monixypAI /mydrive\n","\n","# navigate to the working directory\n","%cd /mydrive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["ln: failed to create symbolic link '/mydrive/monixypAI': File exists\n","/content/gdrive/My Drive/Colab Notebooks/monixypAI\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vj4PG4_i9Alt"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"qGEXPcFJ9UTY"},"source":["Let’s start by checking to see what GPU we’ve been assigned. Ideally we get a V100, but a P100 is fine too. Other GPUs may lead to issues."]},{"cell_type":"code","metadata":{"id":"7VVICTCvd4mc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615043019295,"user_tz":-60,"elapsed":1257,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"a656dfb3-5a7b-450b-a160-4c3e2b9f7b2a"},"source":["!nvidia-smi -L"],"execution_count":3,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-6f0394f7-34db-0eb7-56b0-8b54b15c5118)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rSV_HEoD9dxo"},"source":["Next let’s connect our Google Drive account. This is optional but highly recommended."]},{"cell_type":"code","metadata":{"id":"IuVPuJmbigRs","executionInfo":{"status":"ok","timestamp":1615043019296,"user_tz":-60,"elapsed":1250,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTjVmfSK9CYa"},"source":["## Install repo\n","\n","The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."]},{"cell_type":"code","metadata":{"id":"B8ADVNpBh8Ox","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615043021544,"user_tz":-60,"elapsed":3490,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"da26bdec-d2e0-4698-df91-6c8289754920"},"source":["import os\n","if os.path.isdir(\"/mydrive/stylegan2-ada-pytorch\"):\n","    %cd \"/mydrive/stylegan2-ada-pytorch/\"\n","# elif os.path.isdir(\"/mydrive/\"):\n","#     #install script\n","#     %cd \"/mydrive/\"\n","#     !mkdir colab-sg2-ada-pytorch\n","#     %cd colab-sg2-ada-pytorch\n","#     !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","#     %cd stylegan2-ada-pytorch\n","#     !mkdir downloads\n","#     !mkdir datasets\n","#     !mkdir pretrained\n","#     !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n","# else:\n","elif os.path.isdir(\"/mydrive/\"):\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    %cd pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n","    %cd ../\n","\n","!pip install ninja"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/monixypAI/stylegan2-ada-pytorch\n","Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (1.10.0.post2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9jMmUpn4DWRe"},"source":["You probably don’t need to run this, but this will update your repo to the latest and greatest."]},{"cell_type":"code","metadata":{"id":"uV9bdvzeDRPd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615043022368,"user_tz":-60,"elapsed":4305,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"160e21c7-32a9-4195-945c-681854c14e19"},"source":["%cd \"/mydrive/stylegan2-ada-pytorch/\"\n","!git config --global user.email 'adamsiemaszkiewicz@gmail.com'\n","!git config --global user.name 'adamsiemaszkiewicz'\n","!git fetch origin\n","!git pull\n","!git checkout origin/main -- train.py"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/monixypAI/stylegan2-ada-pytorch\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cZkcJ58P97Ls"},"source":["## Dataset Preparation\n","\n","Upload a .zip of square images to the `datasets` folder. Previously you had to convert your model to .tfrecords. That’s no longer needed :)"]},{"cell_type":"markdown","metadata":{"id":"e_7vpUSE3-vY"},"source":["### Resize images"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baacvkhik9q-","executionInfo":{"status":"ok","timestamp":1615043022614,"user_tz":-60,"elapsed":4542,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"20bcd5e8-ca30-41ab-d227-6bcaf2a79375"},"source":["# !python dataset_tool.py --help \r\n","\r\n","!python /mydrive/stylegan2-ada-pytorch/dataset_tool.py \\\r\n","      --source /mydrive/images/ --dest /mydrive/images_out/ \\\r\n","      --transform=center-crop-wide --width=512 --height=512"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Error: --dest folder must be empty\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j2CG2vZi8JVH","executionInfo":{"status":"ok","timestamp":1615043022614,"user_tz":-60,"elapsed":4533,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}}},"source":["# !python /mydrive/stylegan2-ada-pytorch/train.py --help"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5B-h6FpB9FaK"},"source":["## Train model"]},{"cell_type":"markdown","metadata":{"id":"bNc-3wTO-MUd"},"source":["Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n","\n","* `dataset_path`: this is the path to your .zip file\n","* `resume_from`: if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'`\n","* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."]},{"cell_type":"code","metadata":{"id":"JV0W6yxP-UIn","executionInfo":{"status":"ok","timestamp":1615043167753,"user_tz":-60,"elapsed":553,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}}},"source":["#required: definitely edit these!\n","dataset_path = '/mydrive/images/downscaled.zip'\n","resume_from = '/mydrive/stylegan2-ada-pytorch/pretrained/FreaGAN-10k.pt'\n","aug_strength = 0.0\n","train_count = 0\n","mirror_x = True\n","\n","#broken, don't use for now :(\n","#mirror_y = False\n","\n","#optional: you might not need to edit these\n","gamma_value = 50.0\n","augs = 'bg'\n","config = '11gb-gpu'\n","# config = 'paper512'\n","snapshot_count = 4"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"EL-M7WnnfMDI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615043272192,"user_tz":-60,"elapsed":36032,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}},"outputId":"3fb309bf-01f1-4c85-c7be-86bc8a8668c7"},"source":["!python /mydrive/stylegan2-ada-pytorch/train.py --gpus=1 --cfg=$config \\\r\n","        --metrics=None --outdir=./results --data=$dataset_path \\\r\n","        --snap=$snapshot_count --augpipe=$augs \\\r\n","        --initstrength=$aug_strength --gamma=$gamma_value \\\r\n","        --mirror=$mirror_x --mirrory=False --nkimg=$train_count"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 4,\n","  \"network_snapshot_ticks\": 4,\n","  \"metrics\": [],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/mydrive/images/downscaled.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 144,\n","    \"xflip\": true,\n","    \"resolution\": 512\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 8\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 32768,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 8\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 50.0\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 64,\n","  \"batch_gpu\": 8,\n","  \"ema_kimg\": 20,\n","  \"ema_rampup\": null,\n","  \"nimg\": 0,\n","  \"ada_target\": 0.6,\n","  \"augment_p\": 0.0,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1\n","  },\n","  \"run_dir\": \"./results/00004-downscaled-mirror-paper512-gamma50-bg\"\n","}\n","\n","Output directory:   ./results/00004-downscaled-mirror-paper512-gamma50-bg\n","Training data:      /mydrive/images/downscaled.zip\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   144\n","Image resolution:   512\n","Conditional model:  False\n","Dataset x-flips:    True\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","\n","Num images:  288\n","Image shape: [4, 512, 512]\n","Label shape: [0]\n","\n","Constructing networks...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator             Parameters  Buffers  Output shape        Datatype\n","---                   ---         ---      ---                 ---     \n","mapping.fc0           262656      -        [8, 512]            float32 \n","mapping.fc1           262656      -        [8, 512]            float32 \n","mapping.fc2           262656      -        [8, 512]            float32 \n","mapping.fc3           262656      -        [8, 512]            float32 \n","mapping.fc4           262656      -        [8, 512]            float32 \n","mapping.fc5           262656      -        [8, 512]            float32 \n","mapping.fc6           262656      -        [8, 512]            float32 \n","mapping.fc7           262656      -        [8, 512]            float32 \n","mapping               -           512      [8, 16, 512]        float32 \n","synthesis.b4.conv1    2622465     32       [8, 512, 4, 4]      float32 \n","synthesis.b4.torgb    264708      -        [8, 4, 4, 4]        float32 \n","synthesis.b4:0        8192        16       [8, 512, 4, 4]      float32 \n","synthesis.b4:1        -           -        [8, 512, 4, 4]      float32 \n","synthesis.b8.conv0    2622465     80       [8, 512, 8, 8]      float32 \n","synthesis.b8.conv1    2622465     80       [8, 512, 8, 8]      float32 \n","synthesis.b8.torgb    264708      -        [8, 4, 8, 8]        float32 \n","synthesis.b8:0        -           16       [8, 512, 8, 8]      float32 \n","synthesis.b8:1        -           -        [8, 512, 8, 8]      float32 \n","synthesis.b16.conv0   2622465     272      [8, 512, 16, 16]    float32 \n","synthesis.b16.conv1   2622465     272      [8, 512, 16, 16]    float32 \n","synthesis.b16.torgb   264708      -        [8, 4, 16, 16]      float32 \n","synthesis.b16:0       -           16       [8, 512, 16, 16]    float32 \n","synthesis.b16:1       -           -        [8, 512, 16, 16]    float32 \n","synthesis.b32.conv0   2622465     1040     [8, 512, 32, 32]    float32 \n","synthesis.b32.conv1   2622465     1040     [8, 512, 32, 32]    float32 \n","synthesis.b32.torgb   264708      -        [8, 4, 32, 32]      float32 \n","synthesis.b32:0       -           16       [8, 512, 32, 32]    float32 \n","synthesis.b32:1       -           -        [8, 512, 32, 32]    float32 \n","synthesis.b64.conv0   2622465     4112     [8, 512, 64, 64]    float16 \n","synthesis.b64.conv1   2622465     4112     [8, 512, 64, 64]    float16 \n","synthesis.b64.torgb   264708      -        [8, 4, 64, 64]      float16 \n","synthesis.b64:0       -           16       [8, 512, 64, 64]    float16 \n","synthesis.b64:1       -           -        [8, 512, 64, 64]    float32 \n","synthesis.b128.conv0  1442561     16400    [8, 256, 128, 128]  float16 \n","synthesis.b128.conv1  721409      16400    [8, 256, 128, 128]  float16 \n","synthesis.b128.torgb  132356      -        [8, 4, 128, 128]    float16 \n","synthesis.b128:0      -           16       [8, 256, 128, 128]  float16 \n","synthesis.b128:1      -           -        [8, 256, 128, 128]  float32 \n","synthesis.b256.conv0  426369      65552    [8, 128, 256, 256]  float16 \n","synthesis.b256.conv1  213249      65552    [8, 128, 256, 256]  float16 \n","synthesis.b256.torgb  66180       -        [8, 4, 256, 256]    float16 \n","synthesis.b256:0      -           16       [8, 128, 256, 256]  float16 \n","synthesis.b256:1      -           -        [8, 128, 256, 256]  float32 \n","synthesis.b512.conv0  139457      262160   [8, 64, 512, 512]   float16 \n","synthesis.b512.conv1  69761       262160   [8, 64, 512, 512]   float16 \n","synthesis.b512.torgb  33092       -        [8, 4, 512, 512]    float16 \n","synthesis.b512:0      -           16       [8, 64, 512, 512]   float16 \n","synthesis.b512:1      -           -        [8, 64, 512, 512]   float32 \n","---                   ---         ---      ---                 ---     \n","Total                 30279599    699904   -                   -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape        Datatype\n","---            ---         ---      ---                 ---     \n","b512.fromrgb   320         16       [8, 64, 512, 512]   float16 \n","b512.skip      8192        16       [8, 128, 256, 256]  float16 \n","b512.conv0     36928       16       [8, 64, 512, 512]   float16 \n","b512.conv1     73856       16       [8, 128, 256, 256]  float16 \n","b512           -           16       [8, 128, 256, 256]  float16 \n","b256.skip      32768       16       [8, 256, 128, 128]  float16 \n","b256.conv0     147584      16       [8, 128, 256, 256]  float16 \n","b256.conv1     295168      16       [8, 256, 128, 128]  float16 \n","b256           -           16       [8, 256, 128, 128]  float16 \n","b128.skip      131072      16       [8, 512, 64, 64]    float16 \n","b128.conv0     590080      16       [8, 256, 128, 128]  float16 \n","b128.conv1     1180160     16       [8, 512, 64, 64]    float16 \n","b128           -           16       [8, 512, 64, 64]    float16 \n","b64.skip       262144      16       [8, 512, 32, 32]    float16 \n","b64.conv0      2359808     16       [8, 512, 64, 64]    float16 \n","b64.conv1      2359808     16       [8, 512, 32, 32]    float16 \n","b64            -           16       [8, 512, 32, 32]    float16 \n","b32.skip       262144      16       [8, 512, 16, 16]    float32 \n","b32.conv0      2359808     16       [8, 512, 32, 32]    float32 \n","b32.conv1      2359808     16       [8, 512, 16, 16]    float32 \n","b32            -           16       [8, 512, 16, 16]    float32 \n","b16.skip       262144      16       [8, 512, 8, 8]      float32 \n","b16.conv0      2359808     16       [8, 512, 16, 16]    float32 \n","b16.conv1      2359808     16       [8, 512, 8, 8]      float32 \n","b16            -           16       [8, 512, 8, 8]      float32 \n","b8.skip        262144      16       [8, 512, 4, 4]      float32 \n","b8.conv0       2359808     16       [8, 512, 8, 8]      float32 \n","b8.conv1       2359808     16       [8, 512, 4, 4]      float32 \n","b8             -           16       [8, 512, 4, 4]      float32 \n","b4.mbstd       -           -        [8, 513, 4, 4]      float32 \n","b4.conv        2364416     16       [8, 512, 4, 4]      float32 \n","b4.fc          4194816     -        [8, 512]            float32 \n","b4.out         513         -        [8, 1]              float32 \n","---            ---         ---      ---                 ---     \n","Total          28982913    480      -                   -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Traceback (most recent call last):\n","  File \"/mydrive/stylegan2-ada-pytorch/train.py\", line 566, in <module>\n","    main() # pylint: disable=no-value-for-parameter\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n","    return self.main(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n","    rv = self.invoke(ctx)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n","    return ctx.invoke(self.callback, **ctx.params)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n","    return callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/click/decorators.py\", line 21, in new_func\n","    return f(get_current_context(), *args, **kwargs)\n","  File \"/mydrive/stylegan2-ada-pytorch/train.py\", line 559, in main\n","    subprocess_fn(rank=0, args=args, temp_dir=temp_dir)\n","  File \"/mydrive/stylegan2-ada-pytorch/train.py\", line 408, in subprocess_fn\n","    training_loop.training_loop(rank=rank, **args)\n","  File \"/content/gdrive/My Drive/Colab Notebooks/monixypAI/stylegan2-ada-pytorch/training/training_loop.py\", line 220, in training_loop\n","    grid_size, images, labels = setup_snapshot_image_grid(training_set=training_set)\n","  File \"/content/gdrive/My Drive/Colab Notebooks/monixypAI/stylegan2-ada-pytorch/training/training_loop.py\", line 63, in setup_snapshot_image_grid\n","    images, labels = zip(*[training_set[i] for i in grid_indices])\n","  File \"/content/gdrive/My Drive/Colab Notebooks/monixypAI/stylegan2-ada-pytorch/training/training_loop.py\", line 63, in <listcomp>\n","    images, labels = zip(*[training_set[i] for i in grid_indices])\n","  File \"/content/gdrive/My Drive/Colab Notebooks/monixypAI/stylegan2-ada-pytorch/training/dataset.py\", line 95, in __getitem__\n","    assert list(image.shape) == self.image_shape\n","AssertionError\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RgvSvfyi_R_-"},"source":["### Resume Training\n","\n","Once Colab has shutdown, you’ll need to resume your training. Reset the variables above, particularly the `resume_from` and `aug_strength` settings.\n","\n","1. Point `resume_from` to the last .pkl you trained (you’ll find these in the `results` folder)\n","2. Update `aug_strength` to match the augment value of the last pkl file. Often you’ll see this in the console, but you may need to look at the `log.txt`. Updating this makes sure training stays as stable as possible.\n","3. You may want to update `train_count` to keep track of your training progress.\n","\n","Once all of this has been reset, run that variable cell and the training command cell after it."]},{"cell_type":"markdown","metadata":{"id":"L6EtrPqL9ILk"},"source":["## Testing/Inference\n","\n","TK"]},{"cell_type":"code","metadata":{"id":"VYRXenMoZSHf","executionInfo":{"status":"ok","timestamp":1615043029100,"user_tz":-60,"elapsed":10997,"user":{"displayName":"Adam Siemaszkiewicz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHkDjVXI1qcBXrxIR5B85831M3tmlZou2jukYc-A=s64","userId":"16676861624055550812"}}},"source":["https://github.com/dvschultz/ml-art-colabs\r\n","https://github.com/dvschultz/stylegan2-ada-pytorch\r\n","https://www.youtube.com/watch?v=14JBICMUGfA\r\n","https://github.com/NVlabs/stylegan2-ada-pytorch"],"execution_count":10,"outputs":[]}]}